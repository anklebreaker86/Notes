# 第一章   工欲善其事必先利其器

## 1.1 代码编辑工具Vim

Vim是一款纯命令行操作、功能可扩展、高度可定制的文本编辑工具



1. **安装Vim**

在Ubuntu环境下安装Vim

```c
# apt-get install vim
sudo apt-get install vim
```



查看vim版本号

```
vim -v
```



2. **常用命令**

常见工作模式：

* 普通模式：打开文件时的默认模式，在其他模式下按下ESC键都可返回到该模式
* 插入模式：按i/o/a键进入该模式，进行文本编辑操作，不同之处在于插入字符的位置在光标之前还是之后
* 命令行模式：普通模式下输入冒号（：）后会进入该模式，在该模式下输入命令，如输入：set number或：set nu可以显示行号
* 可视化模式：在普通模式下按v键会进入可视化模式。在该模式下移动光标可以选中一块文本，然后可以进行复制、剪切、删除、粘贴等文本操作
* 替换模式：在普通模式下通过光标选中一个字符，然后按r键，再输入一个字符，你会发现你输入的字符就替换掉了原来那个被选中的字符



单个字符移动：

* k：光标上移一个字符
* j：光标下移一个字符
* h：左移
* l：光标右移



单词移动：

* w：移动到下一个单词开头

* b：上一个单词开头

* e：下一个单词词尾

* ge：上一个单词词尾



行移动：

* $：移动到当前行行尾
* 0：移动到当前行的行首
* ^：移动到当前行的第一个非空字符
* 2|：移动到当前行的第2列
*  fx：移动到当前行的第1个字符x上
* %：符号间的移动，在()、[]、{}之间跳跃



屏幕移动：

* nG：光标跳转到指定的第n行

* gg/G：光标跳转到文件的开头/末尾

* M：光标移动到当前屏幕的中间

* L：光标移动到当前屏幕的末尾

* Ctrl+g：光标查看当前的位置状态

* Ctrl+u/d：光标向前/后半屏滚动

* Ctrl+f/b：光标向前/后全屏滚动

  

















# 第二章   计算机体系结构与CPU工作原理

嵌入式工程师除了要精通C语言编程，还要掌握

* 计算机原理和系统结构：可以更好地理解程序的编译、链接、安装和运行机制
* CPU工作原理：可以更好地理解指令到底是如何执行的
* ARM汇编语言：可以从底层的角度去看C语言，可以帮助我们更好地理解C语言
* 硬件电路



我们编写的C程序，最终都会转换成CPU所支持的二进制指令，而汇编语言又是这些指令集的助记符，通过反汇编代码，我们可以更加深刻地理解编译器的特性和C语言的语法



通过本章学习，对半导体工艺、芯片、CPU、指令集、微架构、计算机系统架构、总线与地址等有一个完整的认知框架



## 2.2 一颗CPU是怎么设计出来的

1. **计算机理论基石：图灵机**

现代计算机理论的技术源头可以追溯到几十年前的图灵机

简单地理解为：任何复杂的运算都可以分解为有限个基本运算指令



图灵机：一条无限长的纸带Tape、一个读写头Head、一套控制规则Table、一个状态寄存器

图灵机内部有一个机器读写头Head，读写头可以一直读取纸带，图灵机根据自己有限的控制规则，根据纸带的输入，不断更新机器的状态，并将输出打印到纸带上

![image-20240209080055872](https://gitea.com/Galaxy/images/raw/branch/main/202402090800252.png)



* 无限长的纸带：相当于程序代码
* 读写头Head：相当于程序计数器PC
* 一套控制规则Table：相当于CPU有限的指令集
* 一个状态寄存器：相当于程序或计算机的状态输出

不同架构的CPU，指令集不同，支持运行的机器指令也不同

但是有一条是相同的：每一种CPU只能支持有限个指令，任何复杂的运算最终都可以分解成有限个基本指令来完成：加、减、乘、除、与、或、非、移位等算术运算或逻辑运算



2. **CPU内部结构及工作原理**

CPU内部构造很简单，只包含基本的算术逻辑运算单元、控制单元、寄存器等，仅支持有限个指令

CPU支持的有限个基本指令集合，称为指令集

程序代码存储在内部存储器（内存）中，CPU可以从内存中一条一条地取指令、翻译指令并执行它

![image-20240209080147476](https://gitea.com/Galaxy/images/raw/branch/main/202402090801154.png)



* 算术逻辑单元（ALU）：是处理器最核心的部件，相当于CPU的大脑
  * ALU由算术单元和逻辑单元组成，算术单元主要负责数学运算，如加、减、乘等；逻辑单元主要负责逻辑运算，如与、或、非等
  * ALU只是纯粹的运算单元，要想完成一个指令运行的整个流程，还需要控制单元的协助
* 控制单元：控制单元根据程序计数器PC中的地址，会不断地从内存RAM中取指令，放到指令寄存器中并进行译码，将指令中的操作码和操作数分别送到ALU，执行相应的运算
  * 例如，整数A和B相加
  * 控制单元通过指令译码电路会将该指令分解为操作码和操作数，再根据操作数地址从内存RAM中加载（Load）数据A和B，传送到ALU的输入端，然后将操作运算类型（操作码）即加法也告诉ALU
  * ALU有了输入数据和操作类型，就可以直接进行相应的运算了，并输出运算结果
  * 为了效率考虑，运算结果一般会先保存到寄存器中，然后由控制单元将该数据从寄存器存储（Store）到内存RAM中
* 程序计数器（PC）：系统上电后默认初始化为0，控制单元会根据这个PC寄存器中的地址到对应的内存RAM中取指令，然后PC寄存器中的地址自动加一



早期CPU的工作频率和内存RAM相比，差距较大

控制单元从RAM中加载数据到CPU，或者将CPU内部的数据存储到RAM中，一般要经过多个时钟读写周期才能完成



为了提高性能，防止RAM拖后腿，CPU一般都会在内部配置一些寄存器，用来保存CPU在计算过程中的各种临时结果和状态值



ALU在运算过程中，当运算结果为0、为负、数据溢出时，也会有一些Flags标志位输出，这些标志位对控制单元特别有用，如一些条件跳转指令，其实就是根据运算结果的这些标志位进行跳转的

CPU跳转指令的实现其实也很简单：根据ALU的运算结果和输出的Flags标志位，直接修改PC寄存器的地址即可



CPU所支持的加、减、乘、与、或、非、跳转、Load/Store、IN/OUT等基本指令，一般称为指令集

任何复杂的运算都可以分解为指令集中的基本指令



为了编程方便，我们给每个二进制指令起一个别名，使用一个助记符表示，这些助记符就是汇编语言，由助记符组成的指令序列就是汇编程序



编译好的高级语言程序通过编译器，就可以翻译成CPU所能识别得二进制机器指令

![image-20240406173827976](https://gitea.com/Galaxy/images/raw/branch/main/202404061738293.png)



3. **CPU设计流程**

略



## 2.3 计算机体系结构

存储器按照存储类型可分为易失性存储器和非易失性存储器

* 易失性存储器如SRAM、DDR SDRAM等，一般用作计算机的内部存储器，所以又被称为内存
  * 这类存储器支持随机访问，CPU可以随机到它的任意地址去读写数据，访问非常方便
  * 但缺点是断电后数据会立即消失，无法永久保存
* 非易失性存储器一般用作计算机的外部存储器，也被称为外存，如磁盘、Flash等
  * 这类存储器支持数据的永久保存，断电后数据也不会消失
  * 但缺点是不支持随机访问，读写速度也不如内存



计算机系统一般会采用内存+外存的存储结构：程序指令保存在诸如磁盘、NAND Flash、SD卡等外部存储器中

当程序运行时，相应的程序会首先加载到内存，然后CPU从内存一条一条地取指令、翻译指令和运行指令



指令和数据都需要保存在存储器中，根据保存方式的不同，计算机可分为两种不同的架构：冯·诺依曼架构和哈弗架构



1. **冯诺伊曼架构**

采用冯·诺依曼架构的计算机，其特点是程序中的指令和数据混合存储，存储在同一块存储器上

程序中的指令和数据同时存放在同一个存储器的不同物理地址上，一般我们会把指令和数据存放到外存储器中

冯·诺依曼架构的特点是结构简单，工程上容易实现，如x86、ARM7、MIPS等



2. **哈弗架构**

哈弗架构的特点是：指令和数据被分开独立存储，它们分别被存放到程序存储器和数据存储器

每个存储器都独立编址，独立访问，而且指令和数据可以在一个时钟周期内并行访问

使用哈弗架构的处理器运行效率更高，但缺点是CPU实现会更加复杂



3. **混合架构**

CPU的频率可以达到GHz级别，而对应的内存RAM一般工作在几百兆赫兹

内存带宽瓶颈会拖慢CPU的工作节奏，进而影响计算机系统的整体运行效率

CPU引入了Cache机制：指令Cache和数据Cache，用来缓存数据和指令，提升计算机的运行效率



现代ARM SoC芯片架构，内部的Cache层采用哈弗架构，集成了指令Cache和数据Cache

而SoC芯片外部则采用冯·诺依曼架构，工程实现简单

![image-20240406181900121](https://gitea.com/Galaxy/images/raw/branch/main/202404061819276.png)



## 2.4 CPU性能提升：Cache机制

CPU的工作频率越来越高，和CPU进行频繁数据交换的内存的运行速度却没有相应提升，两者之间产生了带宽问题

为了不给CPU拖后腿，解决内存带宽瓶颈的方法一般有两个：

* 大幅提升内存RAM的工作频率
* 使用Cache缓存机制



1. **Cache的工作原理**

Cache在物理实现上其实就是静态随机访问存储器（Static Random Access Memory，SRAM）

Cache的运行速度介于CPU和内存DRAM之间，是在CPU和内存之间插入的一组高速缓冲存储器，用来解决两者速度不匹配带来的瓶颈问题



Cache的工作原理很简单，就是利用空间局部性和时间局部性原理，通过自有的存储空间，缓存一部分内存中的指令和数据，减少CPU访问内存的次数，从而提高系统的整体性能



![image-20240617234932642](https://gitea.com/Galaxy/images/raw/branch/main/202406172349134.png)

当CPU读取内存中地址为8的数据时，CPU会将内存中地址为8的一片数据缓存到Cache中

等下一次CPU读取内存地址为12的数据时，会首先到Cache中检查该地址是否在Cache中

如果在，就称为缓存命中（Cache Hit），CPU就直接从Cache中取数据；如果该地址不在Cache中，就称为缓存未命中（Cache Miss），CPU就重新转向内存读取数据，并重新缓存从该地址开始的一片数据到Cache中



![image-20240617235237680](https://gitea.com/Galaxy/images/raw/branch/main/202406172352343.png)

CPU写内存的工作流程和读类似

当CPU往地址为16的内存写入数据0时，并没有真正地写入RAM，而是暂时写到了Cache里

此时Cache和内存RAM的数据就不一致了，缓存的每块空间里一般会有一个特殊的标记位，叫“Dirty Bit”，用来记录这种变化

当Cache需要刷新时，如Cache空间已满而CPU又需要缓存新的数据时，在清理缓存之前，会检查这些“Dirty Bit”标记的变化，并把这些变化的数据回写到RAM中，然后才腾出空间去缓存新的内存数据



2. **一级Cache和二级Cache**

为减少缓存未命中对CPU效率的影响

可以通过增大Cache的容量来提高缓存命中的概率，但随之带来就是成本的上升

既然无法继续增加一级Cache的容量，一个折中的办法就是在一级Cache和内存之间添加二级Cache



现在CPU一般都是多核结构

每个Core都会有自己独立的L1 Cache

在X86架构的CPU中，一般每个Core也会有自己独立的L2 Cache，L3 Cache被所有的Core共享

而在ARM架构的CPU中，L2 Cache则被每簇（Cluster）的Core共享



3. **为什么有些处理器没有Cache**

主要原因有三个：

* 一是这些处理器都是低功耗、低成本处理器，在CPU内集成Cache会增加芯片的面积和发热量，不仅功耗增加，芯片的成本也会增加不少
* 二是这些处理器本来工作频率就不高，和RAM之间不存在带宽问题
* 三是使用Cache无法保证实时性，当缓存未命中时，CPU从RAM中读取数据的时间是不确定的



## 2.5 CPU性能提升：流水线

1. **流水线工作原理**

一条指令的执行一般要经过取指令、翻译指令、执行指令3个基本流程

CPU内部的电路分为不同的单元：取指单元、译码单元、执行单元等

CPU执行指令的3个时钟周期里，取指单元只在第一个时钟周期里工作，其余两个时钟周期都处于空闲状态，其他两个执行单元也是如此



引入流水线后，除了刚开始的第一个时钟周期大家可以偷懒，其余的时间都不能闲着，如图所示：

![image-20240612200210397](https://gitea.com/Galaxy/images/raw/branch/main/202406122002285.png)



原来执行一条指令需要3个时钟周期，引入流水线后平均只需要1个时钟周期，CPU性能提升了不少

流水线的本质其实就是拿空间换时间

将每条指令分解为多步执行，指令的每一小步都有独立的电路单元来执行，并让不同指令的各小步操作重叠，通过多条指令的并行执行，加快程序的整体运行效率



2. **超流水线技术**

流水线存在木桶短板效应，我们只需要找出CPU流水线中的性能瓶颈，即耗时最长的那道工序，对其再进行细分，拆解为更多的工序就可以了

每一道工序都称为流水线中的一级，流水线越深，每一道工序的执行时间就会变得越小，处理器的时钟周期就可以更短，CPU的工作频率就可以更高，进而可以提升CPU的性能，提高工作效率



流水线中耗时最长的那道工序单元的执行时间（即时间延迟）决定了CPU流水线的性能

CPU流水线中的每一级电路单元一般都是由组合逻辑电路和寄存器组成的，组合逻辑电路用来执行本道工序的逻辑运算，寄存器用来保存运算输出结果，并作为下一道工序的输入



CPU内部的数字电路是靠时钟驱动来工作的

我们可以通过缩短一个时钟周期的时间来提升效率，即减少每条指令所耗费的时间

一个时钟周期的时间变短，CPU主频也就相应提升，影响时钟周期时间长短的一个关键的制约因素就是CPU内部每一个工序执行单元的耗费时间



如果驱动CPU工作的时钟周期是2ns，那么CPU的主频就是500MHz

现在的CPU流水线深度可以做到10级以上，流水线的每一级时间延迟都可以做到皮秒级别，驱动CPU工作的时钟周期可以做到更短，可以把CPU的主频飙到5GHz以上



我们把5级以上的流水线称为超流水线结构



要想提升CPU的主频，本质在于减少流水线中每一级流水的执行时间，消除木桶短板效应

解决方法有三个：

* 一是优化流水线中各级流水线的性能，受限于当前集成电路的设计水平，这一步最难
* 二是依靠半导体制造工艺，工艺制程越先进，芯片面积就会越小，发热也就越小，就更容易提升主频
* 三是不断地增加流水线深度，流水线越深，流水线中的各级时间延迟就可以做得越小，就更容易提高主频



流水线的本质是拿空间换时间，流水线越深，电路会越复杂，就需要更多的组合逻辑电路和寄存器，芯片面积也就越大，功耗也就随之上升了



执行的程序指令如果是顺序结构的，没有中断或跳转，流水线确实可以提高执行效率

但是当程序指令中存在跳转、分支结构时，下面预取的指令可能就要全部丢掉了，需要到跳转的地方重新取指令执行



流水线越深，一旦预取指令失败，浪费和损失就会越严重

因为流水线中预取的几十条指令可能都要丢弃掉，此时流水线就发生了停顿，无法按照预期继续执行，这种情况我们一般称为流水线冒险（hazard）



3. **流水线冒险**

一般分为三种：

* 结构冒险：所需的硬件正在为前面的指令工作
* 数据冒险：当前指令需要前面指令的运算数据才能执行
* 控制冒险：需根据之前指令的执行结果决定下一步的行为



结构冒险：

```c
ADD R2,R1,R0
SUB R1,R4,R3
```

上面这两条指令执行时都需要访问寄存器R1，但是这两条指令之间没有依赖关系，不需要数据的传送，仅仅在使用的硬件资源上发生了冲突，这种冲突我们就称为结构冒险

解决结构冒险的方法很简单，我们直接对冲突的寄存器进行重命名就可以了

通过硬件电路对寄存器重命名后，代码就变成了下面的样子，将SUB指令中的R1寄存器重命名为R5，结构冒险解决

```c
ADD R2,R1,R0
SUB R5,R4,R3
```



数据冒险：

```c
ADD R2,R1,R0
SUB R4,R2,R3
```

第二条SUB指令，要等待第一条ADD指令运行结束，将运算结果写回寄存器R2后才能执行

现在的经典CPU流水线一般分为5级：取指、译码、执行、访问内存、写回

也就是说，指令执行结束后还要把运算结果写回寄存器，然后下一条指令才可以到这个寄存器取数据

解决数据冒险：

* 如使用“operand forwarding”技术，当ADD指令运行结束后，不再执行后面的回写寄存器操作，而是直接使用运算结果
* 第二个解决方法是在ADD和SUB指令中间插入空指令，即pipeline bubble，暂缓SUB指令的执行，等ADD指令将运算结果写回寄存器R2后再执行就可以了



控制冒险也是如此，当我们执行BEQ这样的条件判断指令，无法确定接下来要执行什么，无法确定到哪里取指令时，也可以插入几个空指令，等BEQ执行结束后再去取指令就可以了



4. **分支预测**

条件跳转引起的控制冒险虽然也可以通过在流水中插入空泡来避免，但是当流水线很深时，需要插入更多的空泡

以一个20级深度的流水线为例，如果一条指令需要上一条指令执行结束才去执行，则需要在这两条指令之间插入19个空泡，相当于流水线要暂停19个时钟周期，这是CPU无法接受的



现在的CPU流水线在取指和译码时，都要对跳转指令进行分析，预测可能执行的分支和路径，防止预取错误的分支路径指令给流水线带来停顿



分为静态预测和动态预测：

* 静态预测在程序编译时通过编译器进行分支预测
  * 这种预测方式对于循环程序最有效，它可以根据你的循环边界反复取指令
  * 而对于跳转分支，静态预测就比较简单粗暴了，一般都是默认不跳转，按照顺序执行
  * 我们在编写有跳转分支的程序时，要记得把大概率执行的代码分支放在前面，这样可以明显提高代码的执行效率
* 动态预测则指在程序运行时进行预测
  * 1-bit动态预测、n-bit动态预测、下一行预测、双模态预测、局部分支预测、全局分支预测、融合分支预测、循环预测等



分支预测技术是提高CPU性能的一项关键技术，其本质就是去除指令之间的相关性，让程序更高效运行



5. **乱序执行**

我们编写的代码指令序列按照顺序依次存储在RAM中

当程序执行时，PC指针会自动到RAM中去取，然后CPU按照顺序一条一条地依次执行，这种执行方式称为顺序执行（in order）

我们还可以通过乱序执行（out of order）来避免流水线冲突



造成流水线冲突的根源在于指令之间存在相关性：前后指令之间要么产生数据冒险，要么产生结构冒险

我们可以通过重排指令的执行顺序，而不是被动地填充空指令来去掉这种依赖



```c
ADD R2,R1,R0; 指令1
SUB R4,R3,R2; 指令2
ADD R7,R6,R5; 指令3
ADD R10,R9,R8; 指令4
```

在上面的程序中，第二条SUB指令要使用第一条指令的运算结果，于是产生数据冒险

为避免这种情况，可以将指令乱序执行

```c
ADD R2,R1,R0; 指令1
ADD R7,R6,R5; 指令3
ADD R10,R9,R8; 指令4
SUB R4,R3,R2; 指令2
```

因为指令3、指令4和指令1之间不存在相关性，因此我们可将它们放到前面执行

等再次执行到指令2时，指令1已经执行结束，不存在数据冒险，此时我们就不需要在流水线中添加空指令了，CPU流水线满负载运行，效率提升



支持乱序执行的CPU处理器，其内部一般都会有专门的乱序执行逻辑电路，该控制电路会对当前指令的执行序列进行分析，看能否提前执行



CPU分析这些不相关的指令，并结合各电路单元的空闲状态综合判断，将能提前执行的指令进行重排，发送到相应的电路单元执行



6. **SIMD和NEON**

一条指令一般由操作码和操作数构成

当译码电路译码成功并开始执行指令时，CPU的控制单元会首先到内存中取数据，将操作数送到算术逻辑单元中

取数据方法有两种：

* 第一种是先取第一个操作数，然后访问内存读取第二个操作数，最后才能进行求和计算。这种数据操作类型一般称为单指令单数据（Single Instruction Single Data，SISD）
* 第二种方法是几个执行部件同时访问内存，一次性读取所有的操作数，这种数据操作类型称为单指令多数据（Single Instruction Multiple Data，SIMD）



7. **单发射和多发射**

* 单发射处理器：每个时钟周期只能从存储器取一条指令，每个时钟周期也只能执行一条指令
* 多发射处理器：在一个时钟周期内可以执行多条指令



处理器内部一般有多个执行单元，如算术逻辑单元（ALU）、乘法器、浮点运算单元（FPU）等，每个时钟周期内仅有一个执行单元在工作，其他执行单元都闲着



双发射处理器可以在一个时钟周期内同时分发（dispatch）多条指令到不同的执行单元运行，让CPU同时执行不同的计算（加法、乘法、浮点运算等），从而达到指令级的并行

![image-20240621013331070](https://gitea.com/Galaxy/images/raw/branch/main/202406210133187.png)



根据实现方式的不同，多发射处理器又可分为静态发射和动态发射

* 静态发射：在编译阶段将可以并行执行的指令打包，合并到一个64位的长指令中
  * 在打包过程中，若找不到可以并行的指令配对，则用空指令NOP补充。这种实现方式称为超长指令集架构（Very Long Instruction Word，VLIW）
  * VLIW实现简单，不需要额外的硬件，通过编译器在编译阶段就可以完成指令的并行



现在的处理器，如X86、ARM等都采用SuperScalar结构

采用SuperScalar结构的处理器又叫超标量处理器

这种处理器在多发射的实现过程中会增加额外的取指单元、译码单元、逻辑控制单元等硬件电路

在指令运行时，将串行的指令序列转换为并行的指令序列，分发到不同的执行单元去执行，通过指令的动态并行化来提升CPU的性能



## 2.6 多核CPU

1. **单核处理器的瓶颈**

而在相同的工艺下，提升芯片性能和减少功耗之间往往又是冲突的



2. **片上多核互连技术**

单核CPU的性能再强劲，其实也是在串行执行这几个任务，多个任务轮流占用CPU运行

多核处理器则可以让多个任务真正地同时执行



早期的计算机比较简单，CPU和内存、I/O模块直接相连，这种连接也称为星型连接

总线型连接可以随意增加或减少连接模块，兼容性和扩展性都大大增强

![image-20240623104508873](https://gitea.com/Galaxy/images/raw/branch/main/202406231045083.png)



总线型连接也有缺陷，在某一个时刻只允许一对设备进行通信

当多个Core同时想占用总线与外部设备通信时，就会产生竞争，进而影响通信效率

* 一个解决方法是使用线性阵列，分段使用总线，就像高速公路上的不同收费点一样，多个处理器可以分段使用总线资源进行通信，如IBM的Cell处理器
* 另一个解决方法是使用交叉开关（Crossbar）



交叉开关像路由器一样有多个端口，多个Core可以通过交叉开关的端口互连，并行通信

相互通信的各对节点都是独立的，互不干扰

但其自身也会占用芯片面积，功耗很大



因此可以使用层次化交叉开关

通过层次化交叉开关可以在局部构建一个节点的集群，然后在上一层将每个局部的集群看成一个节点，再通过合适的方式进行连接

层次化交叉开关利用网络通信的局部特征，缓解了单个开关在连接的节点上升时产生的性能下降



* 交叉开关两两互连，处理器的多个Core之间通过开关可以相互独立通信（如左侧图）
  * 随着连接节点增多，交叉开关的互连逻辑也越来越复杂，功耗和占用的芯片面积也越来越大

* 四核以上的CPU可以采用Ring Bus结构：将总线和交叉开关结合起来，连成一个环状，相邻的两个Core通信效率最高，远离的两个Core之间可以通过开关路由通信。Intel的八核处理器一般都是采用这种结构的（如右侧图）

![image-20240623105056133](https://gitea.com/Galaxy/images/raw/branch/main/202406231050460.png)



面向众核处理器领域，目前比较流行的一种片上互连技术叫作片上网络（Net On Chip，NoC）

![image-20240623105154315](https://gitea.com/Galaxy/images/raw/branch/main/202406231051572.png)

当处理器的Core很多时，我们不再使用总线型连接，而是使用网络节点的方式连接

每个节点包括计算单元、通信单元及其附属电路

计算和通信实现了分离，每一个节点中的处理单元可以是一个Core，也可以是一个小规模的SoC



3. **big.LITTLE结构**

多个Core集成到一个处理器上，当CPU负载很大时，多个Core一起上阵确实可以提高工作效率

但是当工作任务不是很多时，如只开一个QQ，然后八个核一起跑，只有一个Core在工作，其他Core也开始跟着空转打酱油，随之带来的就是功耗的上升



为避免该情况，推出了big.LITTLE架构：

一个处理器内部集成的有高性能的Core，也有低功耗的Core

当CPU工作负载很重时，启动高性能的Core工作；当CPU很闲时，则切换到低功耗的Core上工作



将所有的高性能核放到一个簇（Cluster）里，构成一个big Cluster，将多个低功耗核放到另一个Cluster里，构成LITTLE Cluster

处理器中的每个Core都有自己独立的数据Cache和指令Cache，每个Cluster共享L2 Cache

为了保证多个Core运行时Cache和RAM中的数据相同，两个Cluster之间通过缓存一致性接口相连，不仅保证多个Core之间的高效通信，还通过检测电路，保证了多个Cache之间、Cache和RAM之间的数据一致性

![image-20240623105755745](https://gitea.com/Galaxy/images/raw/branch/main/202406231057747.png)



4. **超线程技术**

超线程技术（Hyper-Threading，HT）

超线程技术通过增加一定的控制逻辑电路，使用特殊指令可以将一个物理处理器当两个逻辑处理器使用，每个逻辑处理器都可以分配一个线程运行，从而最大限度地提升CPU的资源利用率



在CPU内部很多资源其实也是可以共享的，如ALU、FPU、Cache、总线等

也有很多资源是每个线程独有的，如寄存器状态、堆栈等

我们通过增加一些控制逻辑电路，保存各个线程的状态，共享ALU、Cache等共享资源，就可以在一个物理Core上实现两个逻辑Core，操作系统可以给每个逻辑Core都分配1个线程运行



在同一个物理Core上的两个线程并不是同时运行的，因为每个线程都需要使用物理Core上的共享资源（如ALU、Cache等）

但是两个线程之间可以互相协助运行，一般处理器上的两个线程上下文切换需要20 000个时钟周期，而超线程处理器上的两个线程切换只需要一个时钟周期就可以了



超线程使用条件：

* 首先主板和BIOS要支持超线程技术，操作系统也需要对超线程技术有专门的优化
* 除此之外，应用层面也需要支持超线程技术，如NPTL库等



但在一些对单核性能要求比较高的场合，如大型游戏，开启超线程反而会增加系统开销，影响性能



5. **CPU核数越多越好吗**

CPU的核数不一定越多越好，任务分配不当就可能造成“一核有难，八核围观”的尴尬场面



## 2.7 后摩尔时代：异构计算的崛起

1. **什么是异构计算**

异构计算就是在SoC芯片内部集成不同架构的Core，如DSP、GPU、NPU、TPU等不同架构的处理单元，各个核心协同运算，让整个SoC性能得到充分发挥

CPU像一个大脑，适合处理分支、跳转等复杂逻辑的程序

GPU头脑简单，但四肢发达，擅长处理图片、视频数据

而在人工智能领域，则是NPU和FPGA的战场



2. **GPU**

GPU（Graphic Process Unit，图形处理单元）主要用来处理图像数据

显卡是显式接口卡的简称，计算机联网需要网卡，计算机显示则需要显卡

显卡将数字图像信号转换为模拟信号，并输出到屏幕上



GPU是显卡电路板上的芯片，主要用来进行图像处理、视频渲染

GPU在浮点运算、大数据处理、密码破解、人工智能等领域都是一把好手，比CPU更适合做大规模并行的数据运算



CPU特别擅长处理各种复杂的逻辑程序，如跳转分支、循环结构等

GPU也是一种SIMD结构，但和CPU不同的是，它没有复杂的控制单元和Cache，却集成了几千个，甚至上万个计算核心

GPU天然多线程，特别适合大数据并行处理，在现在的计算机中被广泛使用

在个人计算机上，GPU一般以独立显卡的形式插到主板上，跟CPU一起协同工作；在手机处理器里，GPU一般以IP的形式集成到SoC芯片内部



3. **DSP**

DSP（Digital Signal Processing，数字信号处理器），主要用在音频信号处理和通信领域



4. **FPGA**

FPGA（Field Programmable Gate Array，现场可编程门阵列）

FPGA既解决了定制电路的不足，又克服了原有可编程逻辑器件（Programmable Logic Device，PLD）门电路有限的局限



FPGA芯片内部集成了大量的逻辑门电路和存储器，用户可以通过VHDL、Verilog甚至高级语言编写代码来描述它们之间的连线，将这些连线配置文件写入芯片内部，就可以构成具有特定功能的电路

FPGA直接将硬件描述语言翻译为晶体管门电路的组合，实现特定的算法和功能

可编程逻辑器件通过配套的集成开发工具，可以随时修改代码，下载到芯片内部，重新连线生成新的功能



5. **NPU**

NPU（Neural Network Processing Unit，神经网络处理器）是面向人工智能领域，基于神经网络算法，进行硬件加速的处理器统称



## 2.8 总线与地址

在一个计算机系统中，CPU内部的寄存器是没有地址的，可直接通过寄存器名访问

而内存和外部设备控制器中的寄存器都需要有一个地址，然后CPU才能通过地址去读写这些外部设备控制器的寄存器，控制外部设备的运行，或者根据地址去读写指定的内存单元  



1. **地址的本质**

内存中包含很多存储单元，为了方便管理，我们需要将这些存储单元进行编号管理，每一个存储单元对应一个编号

内存中包含很多存储单元，为了方便管理，我们需要将这些存储单元进行编号管理，每一个存储单元对应一个编号



当CPU想访问其中一个存储单元时，可通过CPU管脚发出一组信号，经过译码器译码，选中与这个信号对应的存储单元，然后就可以直接读写这块内存

CPU管脚发出的这组信号，也就是存储单元对应的编号，即地址



在一个32位的计算机系统中，32位的地址线有4GB大小的寻址空间

寻址空间和一个计算机系统实际的内存大小并不是一回事



在带有MMU的CPU平台下，程序运行一般使用的是虚拟地址

MMU会把虚拟地址转换为物理地址，然后通过CPU管脚发送出去，地址信号通过译码，选中指定的内存存储单元，再进行读写操作



2. **总线的概念**

现在的计算机系统中CPU一般都是通过总线与内存RAM、外部设备相连的

CPU处理器和北桥通过系统总线连接，内存RAM和北桥通过内存总线连接，CPU和各个设备之间可以通过共享总线的方式进行通信



总线其实就是各种数字信号的集合，包括地址信号、数据信号、控制信号等

有的总线还可以为挂到总线上的设备提供电源



一个计算机系统中可能会有各种不同的总线，不同的总线读写时序、工作频率不一样，不同的总线之间通过桥（bridge）来连接

桥一般是一个芯片组电路，用来将总线的电子信号翻译成另一种总线的电子信号



大家生产的设备都采用相同的总线接口，都可以很方便地添加到计算机系统中，不同的设备遵循相同的总线协议与计算机通信



3. **总线编址方式**

计算机一般采用两种编址方式：

* 统一编址：就是内存RAM和外部设备共享CPU的寻址空间，ARM、MIPS架构的CPU都采用这种编址方式
  * 在统一编址模式下，CPU可以像操作内存一样去读写外部设备的寄存器和内部RAM
* 独立编址：内存RAM和外部设备的寄存器独立编址，分别占用不同的地址空间
  * 如X86架构的CPU，外部设备的寄存器有独立的64KB空间，需要专门的IN/OUT指令才能访问，这片独立编址的64KB大小的空间也被称为I/O地址空间



## 2.9 指令集与微架构

不同架构的处理器支持的指令类型是不同的



1. **什么是指令集**

指令集架构（Instruction Set Architecture，ISA）是计算机体系架构的一部分

指令集最终的实现就是微架构，就是CPU内部的各种译码和执行电路



每一种不同架构的CPU一般都需要配套一个对应的编译器



指令集主要由以下内容组成：

* 指令的分发、预取、解码、执行、写回
* 操作数的类型、存储、存取、旁路转移
*  Load/Store架构
*  寄存器
* 地址的格式、大端模式、小端模式
*  字节对齐、边界对齐等



指令集的价值在于大家都遵守同一个标准去开发计算机系统的不同硬件和软件，这非常有利于整个计算机系统生态的构建



1. **什么是微架构**

微架构，对应的英文是Microarchitecture，也就是处理器架构

一套相同的指令集，可以由不同形式的电路实现，可以有不同的微架构

基于一款相同的微架构，通过不同的配置，也可以设计出不同的处理器类型



微架构一般也称为CPU内核

在一个ARM SoC芯片上，我们把CPU内核和各种外设IP通过AMBA总线连接起来，构成一个片上系统，即System On Chip，简称SoC



X86指令集不授权，不开放内核，靠X86专利垄断制造行业壁垒

ARM公司自己不生产CPU，靠IP授权盈利



3. **指令助记符：汇编语言**

一个指令通常由操作码和操作数组成

指令格式是二进制的，就是一串数字，非常不好记，可读性差



为了方便编程，我们给这些二进制指令定义了各种助记符，这种助记符其实就是汇编指令

一段汇编程序经过汇编器的翻译，才能变成CPU真正能识别、译码和运行的二进制指令



学习ARM汇编语言：

* 可以以汇编语言为媒介，深入学习ARM体系架构和CPU内部的工作原理
* 也可以以汇编语言为工具，通过反汇编，深入理解C高级语言



在一些嵌入式软件优化、启动代码、Linux内核OOPS调试等场合，也需要你对汇编语言有一定的掌握

不同的编译器除了支持指令集规定的标准汇编指令，还会自己定义各种伪汇编指令，以方便程序的编写

掌握这些伪指令，对于我们分析计算机底层的工作原理和机制也很有帮助



# 第三章   ARM体系结构与汇编语言

在嵌入式开发领域，ARM架构的处理器占了90%以上的市场份额



## 3.1 ARM体系结构

计算机的指令集一般可分为4种：

* 复杂指令集（CISC）
* 精简指令集（RISC）
* 显式并行指令集（EPIC）
* 超长指令字指令集（VLIW）



嵌入式学习中主要使用RISC，相对于CISC指令集，有以下特点：

*  Load/Store架构：CPU不能直接处理内存中的数据，要先将内存中的数据Load（加载）到寄存器中才能操作，然后将处理结果Store（存储）到内存中
* 固定的指令长度、单周期指令
* 倾向于使用更多的寄存器来存储数据，而不是使用内存中的堆栈，效率更高



ARM的RISC指令集，与原RISC指令集区别：

* ARM有桶型移位寄存器，单周期内可以完成数据的各种移位操作
* 并不是所有的ARM指令都是单周期的
* ARM有16位的Thumb指令集，是32位ARM指令集的压缩形式，提高了代码密度
* 条件执行：通过指令组合，减少了分支指令数目，提高了代码密度
* 增加了DSP、SIMD/NEON等指令



ARM处理器有多种工作模式

应用程序正常运行时，ARM处理器工作在用户模式（User mode）

当程序运行出错或有中断发生时，ARM处理器就会切换到对应的特权工作模式



|   处理器模式    |               模式介绍                |
| :-------------: | :-----------------------------------: |
|    User mode    |        程序正常运行时工作模式         |
|    FIQ mode     |             快速中断模式              |
|    IRQ mode     |               中断模式                |
| Supervisor mode | 管理模式，保护模式，复位和软中断进入  |
|   Abort mode    |   数据存取异常、指令读取失败会进入    |
| Undefined mode  | CPU遇到无法识别、未定义指令时，会进入 |
|   System mode   |   类似用户模式，但可运行特权OS任务    |
|  Monitor mode   |            仅限于安全扩展             |



应用程序正常运行时，处理器处于普通模式，没有权限对内存和底层硬件进行操作

要首先通过系统调用或软中断进入处理器特权模式，运行操作系统内核或硬件驱动代码，才能对底层的硬件设备进行读写操作



在ARM处理器内部，寄存器有：

* 算术运算单元
* 逻辑运算单元
* 浮点运算单元
* 控制单元
* 通用寄存器
* 状态寄存器
* 控制寄存器

控制处理器的运行，保存程序运行时的各种状态和临时结果

![image-20240626213459730](https://gitea.com/Galaxy/images/raw/branch/main/202406262135145.png)



ARM处理器中的寄存器可分为==通用寄存器==和==专用寄存器==两种

寄存器R0～R12属于通用寄存器，除了FIQ工作模式，在其他工作模式下这些寄存器都是共用、共享的：

* R0～R3通常用来传递函数参数
* R4～R11用来保存程序运算的中间结果或函数的局部变量等
* R12常用来作为函数调用过程中的临时寄存器



还有一些寄存器在各自的工作模式下是独立存在的，如R13、R14、R15、CPSP、SPSR寄存器，在每个工作模式下都有自己单独的寄存器

* R13寄存器又称为堆栈指针寄存器（Stack Pointer，SP），用来维护和管理函数调用过程中的栈帧变化，R13总是指向当前正在运行的函数的栈帧，一般不能再用作其他用途
* R14寄存器又称为链接寄存器（Link Register，LR），在函数调用过程中主要用来保存上一级函数调用者的返回地址
* 寄存器R15又称为程序计数器（Program Counter，PC），CPU从内存取指令执行，就是默认从PC保存的地址中取的，每取一次指令，PC寄存器的地址值自动增加



PC指针的值等于当前正在运行的指令地址+8



当前处理器状态寄存器（Current Processor State Register，CPSR）主要用来表征当前处理器的运行状态，其中具有状态位、标志位和控制位



在每种工作模式下，都有一个单独的程序状态保存寄存器（Saved Processor State Register，SPSR）

当ARM处理器切换工作模式或发生异常时，SPSR用来保存当前工作模式下的处理器现场，即将CPSR寄存器的值保存到当前工作模式下的SPSR寄存器

从异常返回时，可切换回原先工作模式



在ARM所有的工作模式中，有一种工作模式比较特殊，即FIQ模式

为了快速响应中断，减少中断现场保护带来的时间开销，在FIQ工作模式下，ARM处理器有自己独享的R8～R12寄存器



## 3.2 ARM汇编指令

一个完整的ARM指令通常由==操作码==+==操作数==组成，指令的编码格式如下

```c
<opcode> {<cond> {s} <Rd>,<Rn> {,<operand2>}}
```



格式具体说明：

* 使用<>标起来的是必选项，使用{}标起来的是可选项
* <opcode>是二进制机器指令的操作码助记符，如MOV、ADD这些汇编指令都是操作码的指令助记符
* cond：执行条件，ARM为减少分支跳转指令个数，允许类似BEQ、BNE等形式的组合指令
* S：是否影响CPSR寄存器中的标志位，如SUBS指令会影响CPSR寄存器中的N、Z、C、V标志位，而SUB指令不会
* Rd：目标寄存器
* Rn：第一个操作数的寄存器
* operand2：第二个可选操作数，灵活使用第二个操作数可以提高代码效率



1. **存储访问指令**

ARM指令集属于RISC指令集，RISC处理器采用典型的加载/存储体系结构，CPU无法对内存里的数据直接操作，只能通过Load/Store指令来实现：

当我们需要对内存中的数据进行操作时，要首先将这个数据从内存加载到寄存器，然后在寄存器中对数据进行处理，最后将结果重新存储到内存中



ARM处理器属于冯·诺依曼架构，程序和数据都存储在同一存储器上，内存空间和I/O空间统一编址，ARM处理器对程序指令、数据、I/O空间中外设寄存器的访问都要通过Load/Store指令来完成



指令使用示例：

```c
LDR R1, [R0]		;将R0中的值作为地址，将该地址上的数据保存到R1
STR R1, [RO]		;将R0中的值作为地址，将R1中的值存储到这个内存地址
LDRB/STRB			;每次读写一字节，LDR/STR默认每次读写4字节
LDM/STM				;批量加载/存储指令，在一组寄存器和一片内存之间传输数据
SWP R1,R1,[R0]		;将R1和R0中地址指向的内存单元中的数据进行交换
SWP R1,R2,[RO]		;将[R0]存储到R1,将R2写入[R0]这个内存存储单元
```



在ARM存储访问指令中，常用的是LDR/STR、LDM/STM这两对指令

LDM/STM指令常用来加载或存储一组寄存器到一片连续的内存

通过和堆栈格式符组合使用，LDM/STM指令还可以用来模拟堆栈操作



不同类型堆栈：

* FA（Full Ascending）：满递增堆栈
* FD（Full Descending）：满递减堆栈
* EA（Empty Ascending）：空递增堆栈
* ED（Empty Descending）：空递减堆栈



 在一个堆栈内存结构中，如果堆栈指针SP总是指向栈顶元素，那么这个栈就是满栈

如果堆栈指针SP指向的是栈顶元素的下一个空闲的存储单元，那么这个栈就是空栈

![image-20240704122410343](https://gitea.com/Galaxy/images/raw/branch/main/202407041224975.png)



如果栈指针SP从高地址往低地址移动，那么这个栈就是递减栈

如果栈指针SP从低地址往高地址移动，那么这个栈就是递增栈

ARM处理器使用的一般都是满递减堆栈



将一组寄存器入栈，或从栈中弹出一组寄存器

```c
LDMFD SP!,{R0-R2,R14}	;将内存栈中的数据依次弹出到R14,R2,R1,R0
STMFD SP!,{R0-R2,R14}	;将R0，R1,R2,R14依次压入内存栈
```



栈元素在入栈操作时，STMFD会根据大括号{}中寄存器列表中各个寄存器的顺序，从左往右依次压入堆栈

栈元素在出栈操作时，顺序刚好相反



ARM还专门提供了PUSH和POP指令来执行栈元素的入栈和出栈操作

```c
PUSH {R0-R2,R14}	;
POP {R0-R2,R14}		;
```



2. **数据传送指令**

LDR/STR指令用来在寄存器和内存之间输送数据

如果我们想要在寄存器之间传送数据，则可以使用MOV指令

```c
MOV {cond} {S} Rd, operand2
```



{cond}为条件指令可选项

{S}表示是否影响CPSR寄存器的值，如MOVS影响，MOV不影响

operand2可以是一个立即数，也可以是一个寄存器



```c
MOV R1, #1		;将立即数1传送到寄存器R1中
MOV R1, R0		;将R0寄存器中的值传送到R1寄存器中
MOV PC, LR		;子程序返回
MVN R0, #OxFF	;将立即数0xFF取反后赋值给R0
MVN R0, R1		;将R1寄存器的值取反后赋值给R0
```



2. **算数逻辑运算指令**

算术运算指令包括加、减、乘、除

逻辑运算指令包括与、或、非、异或、清除等



基本使用说明如下：

```c
ADD R2, R1, #1		;R2=R1+1
ADC R1, R1, #1		;R1=R1+1+C(其中C为CPSR寄存器中进位)
SUB R1, R1, R2		;R1=R1-R2
SBC R1, R1, R2		;R1=R1-R2-C
AND R0, R0, #3		;保留R0的bit0和1，其余位清除
ORR R0, R0, #3		;置位R0的bit0和bit1
EOR R0, R0, #3		;反转R0中的bit0和bit1
BIT R0, R0, #3		;清除R0中的bit0和bit1
```



4. **操作数operand2详解**

operand2：可以是一个常数，也可以是寄存器+偏移的形式

operand2在汇编程序中经常出现的两种格式

```c
#constant
Rm{, shift}
```



第一种格式，操作数是立即数

第二种格式可以直接使用寄存器的值作为操作数，通过{，shift}可选项，还可以通过多种移位或循环移位的方式，构建更加灵活的操作数



可选移位方式如下：

```c
#constant,n		;将立即数constant循环右移n位
ASR #n			;算术右移n位，n取值范围[1,32]
LSL #n			;逻辑左移n位，n取值范围[0,31]
LSR #n			;逻辑右移n位，n取值范围[1,32]
ROR #n 			;向右循环移n位，n取值范围[1,31]
RRX				;向右循环移1位，带扩展
type Rs			;仅在ARM可用，其中type指ASP、LSL、LSR、ROR，Rs是提供位移量的寄存器名称
```

示例如下：

```c
ADD R3, R2, R1, LSL #3	; R3 = R2+R1<<3
ADD R3, R2, R1, LSL, R0	; R3 = R2+R1 <<R0
ADD IP, IP, #16, 20		; IP = IP+立即数16循环右移20位
```



5. **比较指令**

比较指令用来比较两个数的大小，或比较两个数是否相等

比较指令的运算结果会影响CPSR寄存器的N、Z、C、V标志位



比较指令格式如下：

```c
CMP {cond} Rn, operand2	;比较两个数大小
CMN {cond} Rn, operand2	;取负比较
```

使用示例：

```c
CMP R1, #10		; R1-10	，运算结果会影响N、Z、C、V位
CMP R1, R2		; R1-R2，比较结果会影响N、Z、C、V位
CMN	R0, #1		; R0-(-1)，将立即数取负，然后比较大小
```



* 比较指令的运行结果Z=1时，表示运算结果为零，两个数相等

* N=1表示运算结果为负
* N=0表示运算结果为非负，即运算结果为正或者为零



6. **条件执行指令**

BEQ指令表示两个数比较，结果相等时跳转

BNE指令则表示结果不相等时跳转

CPSR寄存器中的标志位根据需要可以任意搭配成不同的条件码，和ARM指令一起组合使用



![image-20240705012421421](https://gitea.com/Galaxy/images/raw/branch/main/202407050124011.png)



可以将无条件跳转指令B和条件码NE组合在一起使用，构成一个循环程序结构

```c
AREA COPY,CODE,READONLY
    ENTRY
START
    LDR R0,=SRC		;源地址
	LDR R1,=DST		;目的地址
	MOV R2,#10		;复制循环次数
LOOP
    LDR R3,[R0],#4	;从源地址取数据
	STR R3,[R1],#4	;复制到目的地址
	SUBS R2,R2,#1	;循环次数减1
	BNE LOOP		;只要R2不等于0，继续循环

AREA COPYDATA,DATA,READWRITE
SRC DCD 1,2,3,4,5,6,7,8,9,0
DST DCD 0,0,0,0,0,0,0,0,0,0
    END
```



7. **跳转指令**

ARM指令集提供

ARM指令集提供了B、BL、BX、BLX等跳转指令，每个指令都有各自的用武之地和使用场景



```c
B {cond} label	;跳转到标号label处执行
B {cond} Rm		;寄存器Rm中保存的是跳转地址
BL {cond} label	
BX {cond} label
BLX {cond} label
```



* B   label

跳转到标号label处，B跳转指令的跳转范围大小为[0，32MB]，可以往前跳，也可以往后跳

无条件跳转指令B主要用在循环、分支结构的汇编程序中

```c
	CMP R2, #0
    BEQ label	;若R2=0，则跳转到label处执行
	...
label
    ...
```



* BL   label

BL跳转指令表示带链接的跳转

在跳转之前，BL指令会先将当前指令的下一条指令地址（即返回地址）保存到LR寄存器中，然后跳转到label处执行



BL指令一般用在函数调用的场合，主函数在跳转到子函数执行之前，会先将返回地址，即当前跳转指令的下一条指令地址保存到LR寄存器中

子函数执行结束后，LR寄存器中的地址被赋值给PC，处理器就可以返回到原来的主函数中继续运行了



```c
//主程序
	...
    BL subfunc	;跳到subfunc执行，在跳之前将返回地址保存在LR
    ...			;子程序返回后接着从此处继续执行
        
//子程序
    ...
    MOV PC,LR	;子程序执行完，将返回地址赋值给PC，返回到主函数

```



* BX   Rm

BX表示带状态切换的跳转

Rm寄存器中保存的是跳转地址，要跳转的目标地址处可能是ARM指令，也可能是Thumb指令

处理器根据Rm[0]位决定是切换到ARM状态还是切换到Thumb状态



* 0：表示目标地址处是ARM指令，在跳转之前要先切换至ARM状态
* 1：表示目标地址处是Thumb指令，在跳转之前要先切换至Thumb状态



BLX指令是BL指令和BX指令的综合，表示带链接和状态切换的跳转，使用方法和上面相同，不再赘述



## 3.3 ARM寻址方式

一个ARM汇编程序中的大部分汇编指令，基本上都和数据传输有关：在内存-寄存器、内存-内存、寄存器-寄存器之间来回传输数据



常见的寻址方：

* 寄存器寻址
* 立即寻址
* 寄存器偏移寻址
* 寄存器间接寻址
* 基址寻址
* 多寄存器寻址
* 相对寻址等



1. **寄存器寻址**

通过寄存器名就可以直接对寄存器中的数据进行读写

```c
MOV R1, R2		;将寄存器R2中的值传送到R1
ADD R1, R2, R3	;运行加法运算R2-R3，将结果保存到R1中
```



2. **立即数寻址**

立即数以＃为前缀，0x前缀表示该立即数为十六进制，不加前缀默认是十进制

```c
ADD R1, R1, #1		;将R1中的值加1，并将结果保存到R1中
MOV R1, #0XFF		;将十六进制常数0xFF写到R1中
ADD R1, R1, #16, 20	;R1 = R1 + 16 循环右移20位
```



3. **寄存器偏移寻址**

通过第二个操作数operand2的灵活配置，我们可以将第二个操作数做各种左移和右移操作，作为新的操作数使用

```c
MOV R2, R1, LSL, #3		;R2 = R1 << 3
ADD R3, R2, R1, LSL #3	;R3 = R2 + R1<<3
ADD R3, R2, R1, LSL R0	;R3 = R2 + R1<<R0
```

逻辑移位：无论是左移还是右移，空缺位一律补0

算数移位：左移时空缺位补0，右移时空缺位使用符号位填充



4. **寄存器间接寻址**

主要用来在内存和寄存器之间传输数据

寄存器中保存的是数据在内存中的存储地址，我们通过这个地址就可以在寄存器和内存之间传输数据

```c
LDR R1, [R2]	;将R2中的值作为地址，取地址上的数据，保存到R1
STR R1, [R2]	;将R2中的值作为地址，将R1寄存器的值写入该地址
```



5. **基址寻址**

基址寻址其实也属于寄存器间接寻址

基址寻址将寄存器中的地址与一个偏移量相加，生成一个新地址，然后基于这个新地址去访问内存

```c
LDR R1,[FP，#2]	;将FP中的值加2作为新地址，取该地址上的值保存到R1
LDR R1,[FP，R0]	;将FP+R0作为新地址，取该地址上的值保存到R1
LDR R1,[FP，R0，LSL #2];将FP+R0<<2作为新地址，读取该内存地址上的值
```



6. **多寄存器寻址**

STM/LDM指令就属于多寄存器寻址，一次可以传输多个寄存器的值

```c
LDMIA SP!, {R0-R2,R14}	;将内存栈中的数据依次弹出到R14，R2,R1,R0
STMDB SP!, {R0-R2,R14}	;将R0,R1,R2,R14依次压入栈
LDMFD SP!, {R0-R2,R14}	;将内存栈中的数据依次弹出到R14,R2,R1,R0
STMFD SP!, {R0-R2,R14}	;将R0,R1,R2,R14依次压入栈
```



LDM/STM指令一般和IA、IB、DA、DB组合使用，分别表示Increase After、Increase Before、Decrease After、Decrease Before

LDM/STM指令也可以和FD、ED、FA、EA组合使用，用于堆栈操作



递增栈A，递减栈D，满栈F，空栈E

ARM中的PUSH和POP指令其实就是LDM/STM的同义词，是LDMFD和STMFD组合指令的助记符



7. **相对寻址**

相对寻址其实也属于基址寻址，只不过它是基址寻址的一种特殊情况

它是以PC指针作为基地址进行寻址的，以指令中的地址差作为偏移，两者相加后得到的就是一个新地址，然后可以对这个地址进行读写操作



ARM中的B、BL、ADR指令其实都是采用相对寻址的



很多与位置无关的代码，如动态链接共享库，其在汇编代码层次的实现其实也是采用相对寻址的



## 3.4 ARM伪指令

伪指令是为了编程方便，各家编译器厂商自定义的一些辅助指令

常见的ARM伪指令主要有4个：

* ADR
* ADRL
* LDR
* NOP



使用示例

```c
ADR R0, LOOP		;将标号LOOP的地址保存到R0寄存器中
ADRL R0, LOOP		;中等范围的地址读取
LDR R0, =0x30008000	;将内存地址0x30008000赋值给R0
NOP					;空操作，用于延时
```

NOP伪指令比较简单，其实就相当于MOV R0，R0



1. **LDR伪指令**

容易和加载指令LDR混淆

LDR伪指令的主要用途是将一个32位的内存地址保存到寄存器中



在寄存器之间传递数据可以使用MOV指令

但是当传递的一个内存地址是32位的立即数时，MOV指令就应付不了了

```c
MOV R0, #200		;往寄存器传递一个立即数，指令正常
MOV R0, #0x30008000	;往寄存器传递一个32位立即数，指令异常
```



RISC指令的特点是单周期指令，指令的长度一般都是固定的

在一个32位的系统中，一条指令通常是32位的，指令中包括操作码和操作数

指令中的操作码和操作数共享32位的存储空间：一般前面的操作码要占据几个比特位，剩下来的留给操作数的编码空间就小于32位了



可以用LDR伪指令

```c
LDR R0, =0x30008000
```

为了与ARM指令集中的加载指令LDR区别开来，LDR伪指令中的操作数前一般会有一个等于号=



编译器在处理伪指令时，根据伪指令中的操作数大小，会使用不同的ARM标准指令替代

如操作数小于8位，一般使用MOV替代；大于8位时，转换为LDR标准指令+文字池



2. **ADR伪指令**

将基于PC相对偏移的地址值读取到寄存器中

ADR为小范围的地址读取伪指令，底层使用相对寻址来实现，因此可以做到代码与位置无关



示例

```c
ADR R0, LOOP
...
...
LOOP
    b LOOP
```

ADR将标号LOOP表征的内存地址送到寄存器R0中

编译时会首先计算出当前正在执行的ADR伪指令地址与标号LOOP之间的地址偏移OFFSET，然后使用ARM指令集中的一条标准指令代替之

```c
OFFSET = LOOP - (PC - 8)
ADD R0, PC, #OFFSET
```



ADR伪指令和LDR伪指令的相似之处：两者都是为了加载一个地址到指定的寄存器中

不同之处：

* LDR伪指令通常被翻译为ARM指令集中的LDR或MOV指令，而ADR伪指令则通常会被ADD或SUB指令代替
* LDR伪指令主要用来操作外部设备的寄存器，而ADR伪指令主要用来通过相对寻址，生成与位置无关的代码
* LDR使用绝对地址，而ADR则使用相对地址
* LDR伪指令适用的地址范围为[0，32GB]，而ADR伪指令则要求当前指令和标号必须在同一个段中，地址偏移范围也较小，地址对齐时偏移范围为[0，1020]，地址未对齐时偏移范围为[0，4096]



## 3.5 ARM汇编程序设计

1. **ARM汇编程序格式**

ARM汇编程序是以段（section）为单位进行组织的

在一个汇编文件中，可以有不同的section，分为代码段、数据段等，各个段之间相互独立，一个ARM汇编程序至少要有一个代码段



我们可以使用AREA伪操作来标识一个段的起始、段名和段的读写属性

```c
AREA COPY,CODE,READONLY		;
	ENTRY
START
	LDR R0,=SRC
    LDR R1,=DST
    MOV R2,#10
LOOP
    LDR R3,[R0],#4
       




```





















# 第四章 程序的编译、链接、安装和运行

IDE集程序编辑器、工程管理器、编译器、汇编器、链接器、调试器、二进制工具、库、头文件于一身，留给用户的使用接口就是创建一个工程，编写代码，运行代码

简化了软件开发



嵌入式开发特点：

* 处理器平台和软件生态碎片化、多样化

* 为了提高性价比，不同的嵌入式系统往往采取更灵活的配置：不同的CPU平台、不同大小的存储、不同的启动方式

导致我们在编译程序时，有时候不仅要考虑一个嵌入式平台的内存、存储器的地址空间，还要考虑将我们的程序代码“烧”写到什么地方、加载到内存什么地方、如何执行

因此需要了解程序是如何编译、链接和运行的



## 4.1 从源程序到二进制文件

程序的编译过程，其实就是将我们编写的C源程序翻译成CPU能够识别和运行的二进制机器指令的过程



```c
//sub.c
int add(int a, int b)
{ return a+b; }

int sub(int a, int b)
{ return a-b; }

//sub.h
int add(int a, int b);
int sub(int a, int b);

//main.c
#include <stdio.h>
#include "sub.h"

int main(void)
{
    int a, b;
    static int local_val = 2;
    static int uninit_local_val;
    a = add(2, 3);
    b = sub(5, 4);
    return 0;
}
```



编译器在编译程序时会根据这些函数声明对我们的源程序进行语法检查：检查实参类型、返回结果类型和函数声明的类型是否匹配



如果我们想让上面的程序在ARM平台上运行，则要使用ARM交叉编译器将C源程序编译生成ARM格式的二进制可执行文件

```c
# arm-linux-gnueabi-gcc -o a.out main.c sub.c
# ./a.
```

















